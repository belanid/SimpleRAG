{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ca82f1",
   "metadata": {},
   "source": [
    "\n",
    "# Create and run a local RAG pipeline from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e919d6f4",
   "metadata": {},
   "source": [
    "## What is RAG ?\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is an approach that combines information retrieval with generative models. In a RAG pipeline, a retriever first searches a large collection of documents to find relevant context for a given query. Then, a generator (such as a language model) uses both the query and the retrieved context to generate a more accurate and informed response. This method enhances the quality of generated answers by grounding them in external knowledge sources.\n",
    "\n",
    "**The 3 main steps in a RAG system are:**\n",
    "1. **Retrieval:** Search a knowledge base or document collection to find passages relevant to the input query.\n",
    "2. **Augmentation:** Combine the retrieved passages with the original query to provide enriched context.\n",
    "3. **Generation:** Use a generative model to produce a response based on the augmented input, leveraging both the query and the retrieved information.\n",
    "\n",
    "https://arxiv.org/abs/2005.11401\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3c9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "GPU Name: NVIDIA GeForce MX450\n",
      "Number of GPUs: 1\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a21fc",
   "metadata": {},
   "source": [
    "## 1 Document processing  and embedding creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04077d4",
   "metadata": {},
   "source": [
    "### 1.1 import PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f296a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "\n",
    "## download the PDF file if it does not exist\n",
    "if not os.path.exists(pdf_path):\n",
    "    url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\" # URL of the PDF file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {pdf_path} from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14592be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace newlines with spaces and trim whitespace at the end of the text.\n",
    "    \"\"\"\n",
    "    return text.replace('\\n', ' ').rstrip()\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Open a PDF file and return a list of dicts with page number as key and page text as value.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for page_num, page in enumerate(tqdm(doc, desc=\"Reading PDF pages\")):\n",
    "        page_text = text_formatter(text=page.get_text())\n",
    "        pages.append({\"page_num\": page_num - 41,\n",
    "                      \"page_char_count\": len(page_text),\n",
    "                      \"page_word_count\": len(page_text.split()),\n",
    "                      \"page_sent_count\": len(page_text.split('. ')),\n",
    "                      \"page_token_count\": len(page_text) / 4,\n",
    "                      \"text\": page_text})\n",
    "    doc.close()\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ac5dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading PDF pages: 100%|██████████| 1208/1208 [00:02<00:00, 494.99it/s]\n"
     ]
    }
   ],
   "source": [
    "pages = open_and_read_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6df7300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_num': 697,\n",
       "  'page_char_count': 1325,\n",
       "  'page_word_count': 202,\n",
       "  'page_sent_count': 12,\n",
       "  'page_token_count': 331.25,\n",
       "  'text': 'Dietary Reference Intake  The IOM has given Adequate Intakes (AI) for fluoride, but has not yet  developed RDAs. The AIs are based on the doses of fluoride shown  to reduce the incidence of cavities, but not cause dental fluorosis.  From infancy to adolescence, the AIs for fluoride increase from 0.01  milligrams per day for ages less than six months to 2 milligrams  per day for those between the ages of fourteen and eighteen. In  adulthood, the AI for males is 4 milligrams per day and for females is  3 milligrams per day. The UL for young children is set at 1.3 and 2.2  milligrams per day for girls and boys, respectively. For adults, the UL  is set at 10 milligrams per day.  Table 11.10 Dietary Reference Intakes for Fluoride  Age Group  AI (mg/day) UL (mg/day)  Infants (0–6 months)  0.01  0.7  Infants (6–12 months)  0.50  0.9  Children (1–3 years)  0.70  1.3  Children (4–8 years)  1.00  2.2  Children (9–13 years)  2.00  10.0  Adolescents (14–18 years)  3.00  10.0  Adult Males (> 19 years)  4.00  10.0  Adult Females (> 19 years) 3.00  10.0  Source: Institute of Medicine. (1997). Dietary Reference Intakes for  Calcium, Phosphorus, Magnesium, Vitamin D, and Fluoride. .  http://www.iom.edu/Reports/1997/Dietary-Reference-Intakes- for-Calcium-Phosphorus-Magnesium-Vitamin-D-and- Fluoride.aspx.  Fluoride  |  697'},\n",
       " {'page_num': 232,\n",
       "  'page_char_count': 1595,\n",
       "  'page_word_count': 216,\n",
       "  'page_sent_count': 15,\n",
       "  'page_token_count': 398.75,\n",
       "  'text': 'carbohydrates are further grouped into the monosaccharides and  disaccharides.  Complex  carbohydrates  are  long  chains  of  monosaccharides.  Simple/Fast-Releasing Carbohydrates  Simple carbohydrates are also known more simply as “sugars” and  are  grouped  as  either  monosaccharides  or  disaccharides.  Monosaccharides include glucose, fructose, and galactose, and the  disaccharides include lactose, maltose, and sucrose.  Simple carbohydrates stimulate the sweetness taste sensation,  which is the most sensitive of all taste sensations. Even extremely  low concentrations of sugars in foods will stimulate the sweetness  taste  sensation.  Sweetness  varies  between  the  different  carbohydrate types—some are much sweeter than others. Fructose  is the top naturally-occurring sugar in sweetness value.  Monosaccharides  For all organisms from bacteria to plants to animals, glucose is the  preferred fuel source. The brain is completely dependent on glucose  as its energy source (except during extreme starvation conditions).  The monosaccharide galactose differs from glucose only in that a  hydroxyl (−OH) group faces in a different direction on the number  four carbon (Figure 4.2 “Structures of the Three Most Common  Monosaccharides: Glucose, Galactose, and Fructose”). This small  structural alteration causes galactose to be less stable than glucose.  As a result, the liver rapidly converts it to glucose. Most absorbed  galactose is utilized for energy production in cells after its  conversion to glucose. (Galactose is one of two simple sugars that  232  |  Introduction'},\n",
       " {'page_num': -26,\n",
       "  'page_char_count': 870,\n",
       "  'page_word_count': 128,\n",
       "  'page_sent_count': 3,\n",
       "  'page_token_count': 217.5,\n",
       "  'text': 'Summary of Major Minerals  University of Hawai‘i at Mānoa Food Science and  Human Nutrition Program and Human Nutrition  Program  645  Part XI. Chapter 11. Trace Minerals  Introduction  University of Hawai‘i at Mānoa Food Science and  Human Nutrition Program and Human Nutrition  Program  651  Iron  University of Hawai‘i at Mānoa Food Science and  Human Nutrition Program and Human Nutrition  Program  655  Copper  University of Hawai‘i at Mānoa Food Science and  Human Nutrition Program and Human Nutrition  Program  666  Zinc  University of Hawai‘i at Mānoa Food Science and  Human Nutrition Program and Human Nutrition  Program  671  Selenium  University of Hawai‘i at Mānoa Food Science and  Human Nutrition Program and Human Nutrition  Program  675  Iodine  University of Hawai‘i at Mānoa Food Science and  Human Nutrition Program and Human Nutrition  Program  681'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.sample(pages, k=3)  # Display 3 random pages from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a5b7ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sent_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-39</td>\n",
       "      <td>320</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00</td>\n",
       "      <td>Human Nutrition: 2020  Edition  UNIVERSITY OF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38</td>\n",
       "      <td>212</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37</td>\n",
       "      <td>797</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>199.25</td>\n",
       "      <td>Contents  Preface  University of Hawai‘i at Mā...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_num  page_char_count  page_word_count  page_sent_count  \\\n",
       "0       -41               29                4                1   \n",
       "1       -40                0                0                1   \n",
       "2       -39              320               42                1   \n",
       "3       -38              212               30                1   \n",
       "4       -37              797              116                3   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0              7.25                      Human Nutrition: 2020 Edition  \n",
       "1              0.00                                                     \n",
       "2             80.00  Human Nutrition: 2020  Edition  UNIVERSITY OF ...  \n",
       "3             53.00  Human Nutrition: 2020 Edition by University of...  \n",
       "4            199.25  Contents  Preface  University of Hawai‘i at Mā...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the list of page dictionaries\n",
    "df_pages = pd.DataFrame(pages)\n",
    "\n",
    "# Display the first\n",
    "df_pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc3e4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sent_count</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50000</td>\n",
       "      <td>1148.016556</td>\n",
       "      <td>171.966060</td>\n",
       "      <td>10.519868</td>\n",
       "      <td>287.004139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86387</td>\n",
       "      <td>560.368736</td>\n",
       "      <td>86.491465</td>\n",
       "      <td>6.548495</td>\n",
       "      <td>140.092184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75000</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>190.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50000</td>\n",
       "      <td>1231.500000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>307.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25000</td>\n",
       "      <td>1603.500000</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>400.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00000</td>\n",
       "      <td>2308.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>577.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         page_num  page_char_count  page_word_count  page_sent_count  \\\n",
       "count  1208.00000      1208.000000      1208.000000      1208.000000   \n",
       "mean    562.50000      1148.016556       171.966060        10.519868   \n",
       "std     348.86387       560.368736        86.491465         6.548495   \n",
       "min     -41.00000         0.000000         0.000000         1.000000   \n",
       "25%     260.75000       762.000000       109.000000         5.000000   \n",
       "50%     562.50000      1231.500000       183.000000        10.000000   \n",
       "75%     864.25000      1603.500000       239.000000        15.000000   \n",
       "max    1166.00000      2308.000000       393.000000        39.000000   \n",
       "\n",
       "       page_token_count  \n",
       "count       1208.000000  \n",
       "mean         287.004139  \n",
       "std          140.092184  \n",
       "min            0.000000  \n",
       "25%          190.500000  \n",
       "50%          307.875000  \n",
       "75%          400.875000  \n",
       "max          577.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pages.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8246ac",
   "metadata": {},
   "source": [
    "### 1.2 Splitting the Pages per Sentence\n",
    "\n",
    "There are two main options for splitting the text in each page into sentences:\n",
    "\n",
    "- **Option 1:** Split by the period character `.` (simple string split).\n",
    "- **Option 2:** Use an NLP library such as spaCy or NLTK for more accurate sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35026d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is the first sentence.,\n",
       " This is the second sentence.,\n",
       " And this is the third.]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "doc = nlp(\"This is the first sentence. This is the second sentence. And this is the third.\")\n",
    "\n",
    "list(doc.sents)  # List of sentences in the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "287331bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|██████████| 1208/1208 [00:02<00:00, 497.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(pages, desc=\"Processing pages\"):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    item[\"sentences\"] = [s.text for s in item[\"sentences\"]]\n",
    "    item[\"sent_count\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a86c8b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_num': 630,\n",
       "  'page_char_count': 75,\n",
       "  'page_word_count': 5,\n",
       "  'page_sent_count': 1,\n",
       "  'page_token_count': 18.75,\n",
       "  'text': 'http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=364    630  |  Calcium',\n",
       "  'sentences': ['http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=364    630  |  Calcium'],\n",
       "  'sent_count': 1}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages, k=1)  # Display 1 random pages with sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8951e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sent_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sent_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "      <td>[Human Nutrition: 2020 Edition]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-39</td>\n",
       "      <td>320</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00</td>\n",
       "      <td>Human Nutrition: 2020  Edition  UNIVERSITY OF ...</td>\n",
       "      <td>[Human Nutrition: 2020  Edition  UNIVERSITY OF...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38</td>\n",
       "      <td>212</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>[Human Nutrition: 2020 Edition by University o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37</td>\n",
       "      <td>797</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>199.25</td>\n",
       "      <td>Contents  Preface  University of Hawai‘i at Mā...</td>\n",
       "      <td>[Contents  Preface  University of Hawai‘i at M...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_num  page_char_count  page_word_count  page_sent_count  \\\n",
       "0       -41               29                4                1   \n",
       "1       -40                0                0                1   \n",
       "2       -39              320               42                1   \n",
       "3       -38              212               30                1   \n",
       "4       -37              797              116                3   \n",
       "\n",
       "   page_token_count                                               text  \\\n",
       "0              7.25                      Human Nutrition: 2020 Edition   \n",
       "1              0.00                                                      \n",
       "2             80.00  Human Nutrition: 2020  Edition  UNIVERSITY OF ...   \n",
       "3             53.00  Human Nutrition: 2020 Edition by University of...   \n",
       "4            199.25  Contents  Preface  University of Hawai‘i at Mā...   \n",
       "\n",
       "                                           sentences  sent_count  \n",
       "0                    [Human Nutrition: 2020 Edition]           1  \n",
       "1                                                 []           0  \n",
       "2  [Human Nutrition: 2020  Edition  UNIVERSITY OF...           1  \n",
       "3  [Human Nutrition: 2020 Edition by University o...           1  \n",
       "4  [Contents  Preface  University of Hawai‘i at M...           2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pages = pd.DataFrame(pages)\n",
    "df_pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d56549ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sent_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>sent_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.02</td>\n",
       "      <td>171.97</td>\n",
       "      <td>10.52</td>\n",
       "      <td>287.00</td>\n",
       "      <td>10.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.37</td>\n",
       "      <td>86.49</td>\n",
       "      <td>6.55</td>\n",
       "      <td>140.09</td>\n",
       "      <td>6.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.00</td>\n",
       "      <td>109.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>190.50</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1231.50</td>\n",
       "      <td>183.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>307.88</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1603.50</td>\n",
       "      <td>239.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>400.88</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_num  page_char_count  page_word_count  page_sent_count  \\\n",
       "count   1208.00          1208.00          1208.00          1208.00   \n",
       "mean     562.50          1148.02           171.97            10.52   \n",
       "std      348.86           560.37            86.49             6.55   \n",
       "min      -41.00             0.00             0.00             1.00   \n",
       "25%      260.75           762.00           109.00             5.00   \n",
       "50%      562.50          1231.50           183.00            10.00   \n",
       "75%      864.25          1603.50           239.00            15.00   \n",
       "max     1166.00          2308.00           393.00            39.00   \n",
       "\n",
       "       page_token_count  sent_count  \n",
       "count           1208.00     1208.00  \n",
       "mean             287.00       10.32  \n",
       "std              140.09        6.30  \n",
       "min                0.00        0.00  \n",
       "25%              190.50        5.00  \n",
       "50%              307.88       10.00  \n",
       "75%              400.88       15.00  \n",
       "max              577.00       28.00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pages.describe().round(2)  # Display summary statistics of the DataFrame rounded to 2 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3854f07",
   "metadata": {},
   "source": [
    "### 1.3 Chuncking sentences together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c34068",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunck_size = 10    # Number of sentences to chunk together\n",
    "def chunk_sentences(sentences:list[str], chunk_size:int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Chunk sentences into groups of a specified size.\n",
    "    \"\"\"\n",
    "    return [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40edb2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       " [30, 31, 32, 33, 34]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = list(range(35))\n",
    "chunk_sentences(test_list, chunck_size)  # Test the chunking function with a list of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f44a2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking sentences: 100%|██████████| 1208/1208 [00:00<00:00, 533563.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sent_count</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>page_chunck_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "      <td>1208.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1148.02</td>\n",
       "      <td>171.97</td>\n",
       "      <td>10.52</td>\n",
       "      <td>287.00</td>\n",
       "      <td>10.32</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86</td>\n",
       "      <td>560.37</td>\n",
       "      <td>86.49</td>\n",
       "      <td>6.55</td>\n",
       "      <td>140.09</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75</td>\n",
       "      <td>762.00</td>\n",
       "      <td>109.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>190.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50</td>\n",
       "      <td>1231.50</td>\n",
       "      <td>183.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>307.88</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25</td>\n",
       "      <td>1603.50</td>\n",
       "      <td>239.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>400.88</td>\n",
       "      <td>15.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2308.00</td>\n",
       "      <td>393.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_num  page_char_count  page_word_count  page_sent_count  \\\n",
       "count   1208.00          1208.00          1208.00          1208.00   \n",
       "mean     562.50          1148.02           171.97            10.52   \n",
       "std      348.86           560.37            86.49             6.55   \n",
       "min      -41.00             0.00             0.00             1.00   \n",
       "25%      260.75           762.00           109.00             5.00   \n",
       "50%      562.50          1231.50           183.00            10.00   \n",
       "75%      864.25          1603.50           239.00            15.00   \n",
       "max     1166.00          2308.00           393.00            39.00   \n",
       "\n",
       "       page_token_count  sent_count  page_chunck_count  \n",
       "count           1208.00     1208.00            1208.00  \n",
       "mean             287.00       10.32               1.53  \n",
       "std              140.09        6.30               0.64  \n",
       "min                0.00        0.00               0.00  \n",
       "25%              190.50        5.00               1.00  \n",
       "50%              307.88       10.00               1.00  \n",
       "75%              400.88       15.00               2.00  \n",
       "max              577.00       28.00               3.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in tqdm(pages, desc=\"Chunking sentences\"):\n",
    "    item[\"chunks\"] = chunk_sentences(item[\"sentences\"], chunck_size)\n",
    "    item[\"page_chunck_count\"] = len(item[\"chunks\"])\n",
    "\n",
    "\n",
    "df_pages = pd.DataFrame(pages)\n",
    "df_pages.describe().round(2)  # Display summary statistics of the DataFrame rounded to 2 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d68bf",
   "metadata": {},
   "source": [
    "### 1.4 Splitting each chunk into its own item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec5db363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting chunks into items: 100%|██████████| 1208/1208 [00:00<00:00, 51048.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks created: 1843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def split_chunks_into_items(pages: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Split each chunk in the pages into separate items.\n",
    "    Each item will contain the chunk text and metadata from the original page.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for page in tqdm(pages, desc=\"Splitting chunks into items\"):\n",
    "        page_num = page[\"page_num\"]\n",
    "        for chunk_idx, chunk in enumerate(page[\"chunks\"]):\n",
    "            chunk_text = \" \".join(chunk).strip()\n",
    "            chunk_metadata = {\n",
    "                \"page_num\": page_num,\n",
    "                \"chunk_idx\": chunk_idx,\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"chunk_char_count\": len(chunk_text),\n",
    "                \"chunk_word_count\": len(chunk_text.split()),\n",
    "                \"chunk_sent_count\": len(chunk),\n",
    "                \"chunk_token_count\": len(chunk_text) / 4,\n",
    "                \"text\": chunk_text,\n",
    "                \"sentences\": chunk\n",
    "            }\n",
    "            items.append(chunk_metadata)\n",
    "    return items\n",
    "\n",
    "# Example usage\n",
    "chunk_items = split_chunks_into_items(pages)\n",
    "print(f\"Total number of chunks created: {len(chunk_items)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c342a771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_num': 645,\n",
       "  'chunk_idx': 0,\n",
       "  'chunk_size': 1,\n",
       "  'chunk_char_count': 206,\n",
       "  'chunk_word_count': 33,\n",
       "  'chunk_sent_count': 1,\n",
       "  'chunk_token_count': 51.5,\n",
       "  'text': 'Summary of Major Minerals  UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN  NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM  Table 10.8 A Summary of the Major Minerals  Summary of Major Minerals  |  645',\n",
       "  'sentences': ['Summary of Major Minerals  UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN  NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM  Table 10.8 A Summary of the Major Minerals  Summary of Major Minerals  |  645']},\n",
       " {'page_num': 506,\n",
       "  'chunk_idx': 1,\n",
       "  'chunk_size': 7,\n",
       "  'chunk_char_count': 350,\n",
       "  'chunk_word_count': 35,\n",
       "  'chunk_sent_count': 7,\n",
       "  'chunk_token_count': 87.5,\n",
       "  'text': 'http://www.health.gov/paguidelines/guidelines/ chapter2.aspx. Published 2008. Accessed September 22,  2017.  8. Source: 2008 Physical Activity Guidelines for Americans.  US Department of Health and Human Services.  http://www.health.gov/paguidelines/guidelines/ 506  |  Dietary, Behavioral, and Physical Activity Recommendations for Weight Management',\n",
       "  'sentences': [' http://www.health.gov/paguidelines/guidelines/ chapter2.aspx.',\n",
       "   'Published 2008.',\n",
       "   'Accessed September 22,  2017.',\n",
       "   ' 8.',\n",
       "   'Source: 2008 Physical Activity Guidelines for Americans.',\n",
       "   ' US Department of Health and Human Services.',\n",
       "   ' http://www.health.gov/paguidelines/guidelines/ 506  |  Dietary, Behavioral, and Physical Activity Recommendations for Weight Management']},\n",
       " {'page_num': 37,\n",
       "  'chunk_idx': 1,\n",
       "  'chunk_size': 5,\n",
       "  'chunk_char_count': 464,\n",
       "  'chunk_word_count': 71,\n",
       "  'chunk_sent_count': 5,\n",
       "  'chunk_token_count': 116.0,\n",
       "  'text': 'Where is your money invested?  What college do you attend?  Evidence-Based Approach to Nutrition  It took more than one hundred years from iodine’s discovery as  an effective treatment for goiter until public health programs  recognized it as such. Although a lengthy process, the scientific  method is a productive way to define essential nutrients and  determine their ability to promote health and prevent disease. The  Research and the Scientific Method  |  37',\n",
       "  'sentences': ['Where is your money invested?',\n",
       "   ' What college do you attend?',\n",
       "   ' Evidence-Based Approach to Nutrition  It took more than one hundred years from iodine’s discovery as  an effective treatment for goiter until public health programs  recognized it as such.',\n",
       "   'Although a lengthy process, the scientific  method is a productive way to define essential nutrients and  determine their ability to promote health and prevent disease.',\n",
       "   'The  Research and the Scientific Method  |  37']}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a few sample chunks\n",
    "import random\n",
    "random.sample(chunk_items, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34081ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>chunk_idx</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_sent_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "      <td>1843.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>583.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.76</td>\n",
       "      <td>752.12</td>\n",
       "      <td>112.85</td>\n",
       "      <td>6.76</td>\n",
       "      <td>188.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>347.79</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3.30</td>\n",
       "      <td>456.24</td>\n",
       "      <td>71.28</td>\n",
       "      <td>3.30</td>\n",
       "      <td>114.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>280.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>322.50</td>\n",
       "      <td>45.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>80.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>765.00</td>\n",
       "      <td>115.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>191.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>890.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1139.50</td>\n",
       "      <td>173.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>284.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1871.00</td>\n",
       "      <td>298.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>467.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_num  chunk_idx  chunk_size  chunk_char_count  chunk_word_count  \\\n",
       "count   1843.00    1843.00     1843.00           1843.00           1843.00   \n",
       "mean     583.38       0.40        6.76            752.12            112.85   \n",
       "std      347.79       0.56        3.30            456.24             71.28   \n",
       "min      -41.00       0.00        1.00             14.00              3.00   \n",
       "25%      280.50       0.00        4.00            322.50             45.00   \n",
       "50%      586.00       0.00        8.00            765.00            115.00   \n",
       "75%      890.00       1.00       10.00           1139.50            173.00   \n",
       "max     1166.00       2.00       10.00           1871.00            298.00   \n",
       "\n",
       "       chunk_sent_count  chunk_token_count  \n",
       "count           1843.00            1843.00  \n",
       "mean               6.76             188.03  \n",
       "std                3.30             114.06  \n",
       "min                1.00               3.50  \n",
       "25%                4.00              80.62  \n",
       "50%                8.00             191.25  \n",
       "75%               10.00             284.88  \n",
       "max               10.00             467.75  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks = pd.DataFrame(chunk_items)\n",
    "df_chunks.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dda7cfb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>chunk_sent_count</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>18.25</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>Published August 2011. Accessed September 22, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>17.25</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>Table 4.6 Sweeteners  Carbohydrates and Person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>25.50</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>view it online here:  http://pressbooks.oer.ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>20.00</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>http://pressbooks.oer.hawaii.edu/ humannutriti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>16.75</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>Accessed January 20, 2018.  The Effect of New ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      chunk_token_count  chunk_sent_count  chunk_char_count  \\\n",
       "250               18.25                 3                73   \n",
       "462               17.25                 1                69   \n",
       "252               25.50                 1               102   \n",
       "1540              20.00                 1                80   \n",
       "1613              16.75                 2                67   \n",
       "\n",
       "                                                   text  \n",
       "250   Published August 2011. Accessed September 22, ...  \n",
       "462   Table 4.6 Sweeteners  Carbohydrates and Person...  \n",
       "252   view it online here:  http://pressbooks.oer.ha...  \n",
       "1540  http://pressbooks.oer.hawaii.edu/ humannutriti...  \n",
       "1613  Accessed January 20, 2018.  The Effect of New ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out chunks with less than 30 tokens\n",
    "token_count_threshold = 30\n",
    "rows_with_few_tokens = df_chunks[df_chunks['chunk_token_count'] < 30]\n",
    "rows_with_few_tokens.sample(5)[[\"chunk_token_count\", \"chunk_sent_count\", \"chunk_char_count\",\"text\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cafdec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks after filtering: 1687\n"
     ]
    }
   ],
   "source": [
    "# Filter out the chunk dictionaries with token count less than the threshold\n",
    "filtered_chunk_items = [item for item in chunk_items if item['chunk_token_count'] >= token_count_threshold]\n",
    "print(f\"Number of chunks after filtering: {len(filtered_chunk_items)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07053aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>chunk_idx</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_sent_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1687.00</td>\n",
       "      <td>1687.00</td>\n",
       "      <td>1687.00</td>\n",
       "      <td>1687.00</td>\n",
       "      <td>1687.00</td>\n",
       "      <td>1687.00</td>\n",
       "      <td>1687.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>580.14</td>\n",
       "      <td>0.35</td>\n",
       "      <td>7.24</td>\n",
       "      <td>815.16</td>\n",
       "      <td>122.40</td>\n",
       "      <td>7.24</td>\n",
       "      <td>203.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>350.06</td>\n",
       "      <td>0.52</td>\n",
       "      <td>3.03</td>\n",
       "      <td>424.68</td>\n",
       "      <td>66.86</td>\n",
       "      <td>3.03</td>\n",
       "      <td>106.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-39.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>276.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>426.50</td>\n",
       "      <td>62.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>106.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>579.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>836.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>209.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>888.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1166.50</td>\n",
       "      <td>177.50</td>\n",
       "      <td>10.00</td>\n",
       "      <td>291.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1871.00</td>\n",
       "      <td>298.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>467.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_num  chunk_idx  chunk_size  chunk_char_count  chunk_word_count  \\\n",
       "count   1687.00    1687.00     1687.00           1687.00           1687.00   \n",
       "mean     580.14       0.35        7.24            815.16            122.40   \n",
       "std      350.06       0.52        3.03            424.68             66.86   \n",
       "min      -39.00       0.00        1.00            120.00             10.00   \n",
       "25%      276.50       0.00        5.00            426.50             62.00   \n",
       "50%      579.00       0.00        8.00            836.00            125.00   \n",
       "75%      888.50       1.00       10.00           1166.50            177.50   \n",
       "max     1166.00       2.00       10.00           1871.00            298.00   \n",
       "\n",
       "       chunk_sent_count  chunk_token_count  \n",
       "count           1687.00            1687.00  \n",
       "mean               7.24             203.79  \n",
       "std                3.03             106.17  \n",
       "min                1.00              30.00  \n",
       "25%                5.00             106.62  \n",
       "50%                8.00             209.00  \n",
       "75%               10.00             291.62  \n",
       "max               10.00             467.75  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks_filtered = df_chunks[df_chunks['chunk_token_count'] >= token_count_threshold]\n",
    "df_chunks_filtered.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7beb5",
   "metadata": {},
   "source": [
    "### 1.5 Converting the dictionary of chunks into numerical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07750c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f013daebb12438b9c055c4c121c69a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings for 3 chunks. Each embedding has shape: 768\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cpu\")\n",
    "\n",
    "# test the embedder on a sample of 3 chunks\n",
    "sample_size = min(3, len(filtered_chunk_items))\n",
    "sample_chunks = random.sample(filtered_chunk_items, k=sample_size)\n",
    "sample_texts = [item['text'] for item in sample_chunks]\n",
    "\n",
    "# Compute embeddings for the sample texts\n",
    "sample_embeddings = embedder.encode(sample_texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "print(f\"Computed embeddings for {len(sample_embeddings)} chunks. Each embedding has shape: {sample_embeddings.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec162139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b032ce5e1724ca0928505547ac23dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 22.7 s\n",
      "Wall time: 2min 22s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchunck_text = [item[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mtext\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m] for item in filtered_chunk_items]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43membedder.to(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43membeddings = embedder.encode(chunck_text, show_progress_bar=True, convert_to_tensor=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# print the shape of the embeddings\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(embeddings.shape)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nidha\\OneDrive\\Documents\\Projects\\simple-local-rag\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nidha\\OneDrive\\Documents\\Projects\\simple-local-rag\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1470\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1469\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1471\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nidha\\OneDrive\\Documents\\Projects\\simple-local-rag\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1434\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1432\u001b[39m st = clock2()\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1436\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:4\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nidha\\OneDrive\\Documents\\Projects\\simple-local-rag\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:346\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[39m\n\u001b[32m    344\u001b[39m sentences_batch = sentences_sorted[start_index : start_index + batch_size]\n\u001b[32m    345\u001b[39m features = \u001b[38;5;28mself\u001b[39m.tokenize(sentences_batch)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m features = \u001b[43mbatch_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m features.update(extra_features)\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nidha\\OneDrive\\Documents\\Projects\\simple-local-rag\\.venv\\Lib\\site-packages\\sentence_transformers\\util.py:357\u001b[39m, in \u001b[36mbatch_to_device\u001b[39m\u001b[34m(batch, target_device)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m         batch[key] = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chunck_text = [item['text'] for item in filtered_chunk_items]\n",
    "\n",
    "embedder.to(\"cuda\")\n",
    "embeddings = embedder.encode(chunck_text, show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "# print the shape of the embeddings\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embeddings to a file\n",
    "# Save the embeddings tensor to a file using torch\n",
    "output_embeddings_path = \"embeddings.pt\"\n",
    "torch.save(embeddings, output_embeddings_path)\n",
    "print(f\"Embeddings saved to {output_embeddings_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6aa26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunck_idx, item in enumerate(filtered_chunk_items):\n",
    "    item[\"embedding\"] = np.array(embeddings[chunck_idx].cpu())\n",
    "\n",
    "df_chunks_filtered = pd.DataFrame(filtered_chunk_items)\n",
    "df_chunks_filtered.head()\n",
    "\n",
    "df_chunks_filtered.to_csv(\"df_chunks_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3cb88ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading PDF pages: 100%|██████████| 1208/1208 [00:01<00:00, 608.90it/s]\n",
      "Processing pages: 100%|██████████| 1208/1208 [00:03<00:00, 352.48it/s]\n",
      "Chunking sentences: 100%|██████████| 1208/1208 [00:00<00:00, 411995.38it/s]\n",
      "Splitting chunks into items: 100%|██████████| 1208/1208 [00:00<00:00, 30336.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding arrays ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2915b7c6e3a41488e25f23516afa0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding embeddings to dictionnary: 1687it [00:00, 8132.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Time taken: 00:07:08.716\n"
     ]
    }
   ],
   "source": [
    "from pdf_ragger import create_embedding_from_pdf\n",
    "\n",
    "pdf_chunk = create_embedding_from_pdf(r\"C:\\Users\\nidha\\OneDrive\\Documents\\Projects\\simple-local-rag\\human-nutrition-text.pdf\", chunk_size=10, token_count_threshold=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b11df",
   "metadata": {},
   "source": [
    "if the embeddings is quite large a better option is to save it as a vector database (to explore later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248da09a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
